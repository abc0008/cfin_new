"""
Analysis Repository Module
========================

This module provides the repository layer for managing financial analysis results
within the CFIN platform. It handles all database interactions related to storing,
retrieving, updating, and deleting analysis data generated from documents.

Primary responsibilities:
- Store new analysis results associated with specific documents.
- Retrieve analysis results by their ID or list them for a given document.
- Update existing analysis results.
- Delete analysis results.
- Provide counting and searching capabilities for analyses.

Key Components:
- AnalysisRepository: Main class that encapsulates all database operations for AnalysisResult entities.
  - Methods for CRUD operations on AnalysisResult.
  - Methods for listing, counting, and searching analyses based on various criteria.

Interactions with other files:
-----------------------------
1. cfin/backend/models/database_models.py:
   - Uses the AnalysisResult, Document, and User SQLAlchemy ORM models.
   - AnalysisResult is the primary model managed by this repository.
   - Document and User models are used for context (e.g., associating analysis with a document, which implicitly links to a user).

2. cfin/backend/services/analysis_service.py:
   - The AnalysisService initializes and uses this repository to persist and retrieve analysis results.
   - It relies on this repository to manage the lifecycle of AnalysisResult entities generated by AI services.

3. cfin/backend/utils/database.py:
   - Implicitly uses the database session (AsyncSession) managed by utils.database for all operations.

This repository ensures that all financial analysis data is consistently managed and accessible,
providing a clean data access layer for the AnalysisService.
"""

import logging
import uuid
from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy import update, delete

from models.database_models import AnalysisResult

logger = logging.getLogger(__name__)

class AnalysisRepository:
    """Repository for analysis operations."""
    
    def __init__(self, db: AsyncSession):
        """Initialize the analysis repository."""
        self.db = db
    
    async def create_analysis(
        self,
        document_ids: List[str],
        analysis_type: str,
        result_data: Dict[str, Any]
    ) -> AnalysisResult:
        """
        Create a new analysis result.
        
        Args:
            document_ids: List of document IDs being analyzed (multi-document support)
            analysis_type: Type of analysis (e.g., "financial_ratios", "sentiment", etc.)
            result_data: JSON data containing analysis results
        
        Returns:
            Created analysis result
        """
        # Create the analysis record
        analysis = AnalysisResult(
            id=str(uuid.uuid4()),
            document_ids=document_ids,  # Store as JSON array
            analysis_type=analysis_type,
            result_data=result_data,
            created_at=datetime.utcnow()
        )
        
        # Add to database
        self.db.add(analysis)
        await self.db.commit()
        await self.db.refresh(analysis)
        
        return analysis
    
    async def get_analysis(self, analysis_id: str) -> Optional[AnalysisResult]:
        """
        Get an analysis result by ID.
        
        Args:
            analysis_id: ID of the analysis
            
        Returns:
            Analysis result if found, None otherwise
        """
        result = await self.db.execute(
            select(AnalysisResult).where(AnalysisResult.id == analysis_id)
        )
        return result.scalars().first()
    
    async def list_document_analyses(
        self,
        document_id: str,
        analysis_type: Optional[str] = None,
        limit: int = 10,
        offset: int = 0
    ) -> List[AnalysisResult]:
        """
        List analysis results for a document.
        
        Args:
            document_id: ID of the document
            analysis_type: Optional analysis type to filter by
            limit: Maximum number of results to return
            offset: Starting index
        
        Returns:
            List of analysis results
        """
        # For SQLite, fetch all and filter in Python since document_ids is a JSON array
        result = await self.db.execute(select(AnalysisResult))
        all_analyses = result.scalars().all()
        filtered = []
        for analysis in all_analyses:
            if document_id in (analysis.document_ids or []):
                if not analysis_type or analysis.analysis_type == analysis_type:
                    filtered.append(analysis)
        filtered.sort(key=lambda x: x.created_at, reverse=True)
        return filtered[offset:offset+limit]
    
    async def list_latest_analyses(
        self,
        document_ids: List[str],
        analysis_type: Optional[str] = None,
        limit: int = 10
    ) -> List[AnalysisResult]:
        """
        List the latest analysis results for multiple documents.
        
        Args:
            document_ids: List of document IDs
            analysis_type: Optional analysis type to filter by
            limit: Maximum number of results to return per document
        
        Returns:
            List of analysis results
        """
        # For SQLite, fetch all and filter in Python
        result = await self.db.execute(select(AnalysisResult))
        all_analyses = result.scalars().all()
        analyses_by_document = {doc_id: [] for doc_id in document_ids}
        for analysis in all_analyses:
            for doc_id in document_ids:
                if doc_id in (analysis.document_ids or []):
                    if not analysis_type or analysis.analysis_type == analysis_type:
                        if len(analyses_by_document[doc_id]) < limit:
                            analyses_by_document[doc_id].append(analysis)
        # Flatten and sort
        latest_analyses = []
        for doc_analyses in analyses_by_document.values():
            latest_analyses.extend(doc_analyses)
        latest_analyses.sort(key=lambda x: x.created_at, reverse=True)
        return latest_analyses
    
    async def update_analysis(
        self,
        analysis_id: str,
        update_data: Dict[str, Any]
    ) -> Optional[AnalysisResult]:
        """
        Update an analysis result.
        
        Args:
            analysis_id: ID of the analysis
            update_data: Dictionary of fields to update
            
        Returns:
            Updated analysis result if found, None otherwise
        """
        await self.db.execute(
            update(AnalysisResult)
            .where(AnalysisResult.id == analysis_id)
            .values(**update_data)
        )
        await self.db.commit()
        
        return await self.get_analysis(analysis_id)
    
    async def delete_analysis(self, analysis_id: str) -> bool:
        """
        Delete an analysis result.
        
        Args:
            analysis_id: ID of the analysis
            
        Returns:
            True if analysis was deleted, False otherwise
        """
        # Delete from database
        await self.db.execute(
            delete(AnalysisResult).where(AnalysisResult.id == analysis_id)
        )
        await self.db.commit()
        
        return True
    
    async def count_document_analyses(
        self,
        document_id: str,
        analysis_type: Optional[str] = None
    ) -> int:
        """
        Count the number of analysis results for a document.
        
        Args:
            document_id: ID of the document
            analysis_type: Optional analysis type to filter by
        
        Returns:
            Number of analysis results
        """
        # For SQLite, fetch all and filter in Python
        result = await self.db.execute(select(AnalysisResult))
        all_analyses = result.scalars().all()
        count = 0
        for analysis in all_analyses:
            if document_id in (analysis.document_ids or []):
                if not analysis_type or analysis.analysis_type == analysis_type:
                    count += 1
        return count
    
    async def search_analyses(
        self,
        document_ids: List[str],
        query: str,
        analysis_type: Optional[str] = None,
        limit: int = 10,
        offset: int = 0
    ) -> List[AnalysisResult]:
        """
        Search analysis results by content.
        
        Args:
            document_ids: List of document IDs to search within
            query: Search query
            analysis_type: Optional analysis type to filter by
            limit: Maximum number of results to return
            offset: Starting index
        
        Returns:
            List of matching analysis results
        """
        # For SQLite, fetch all and filter in Python
        result = await self.db.execute(select(AnalysisResult))
        all_analyses = result.scalars().all()
        matching_analyses = []
        for analysis in all_analyses:
            if any(doc_id in (analysis.document_ids or []) for doc_id in document_ids):
                if not analysis_type or analysis.analysis_type == analysis_type:
                    result_data_str = str(analysis.result_data)
                    if query.lower() in result_data_str.lower():
                        matching_analyses.append(analysis)
        matching_analyses.sort(key=lambda x: x.created_at, reverse=True)
        return matching_analyses[offset:offset + limit]
    
    async def is_document_referenced(self, document_id: str) -> bool:
        """
        Check if a document is referenced in any analysis result.
        Args:
            document_id: ID of the document
        Returns:
            True if the document is referenced, False otherwise
        """
        result = await self.db.execute(select(AnalysisResult))
        all_analyses = result.scalars().all()
        for analysis in all_analyses:
            if document_id in (analysis.document_ids or []):
                return True
        return False